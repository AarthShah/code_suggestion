# ğŸš€ AI-Powered Code Suggestion System# Code Suggestion System - Quick Start Guide



> An intelligent Python code completion system powered by Hybrid N-gram + LSTM neural network with smart template detection## ğŸ¯ Your Model is Ready!



[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)**Training Results:**

[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-red.svg)](https://pytorch.org/)- âœ… Dataset: 18,612 Python code samples

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)- âœ… N-gram size: 15 (maximum context)

- âœ… LSTM: 256 hidden units, 26.7M parameters

## ğŸ“– Overview- âœ… Training time: 51 minutes on RTX 4060

- âœ… Final validation loss: 3.89 (excellent!)

This project implements a state-of-the-art code suggestion system that combines:

- **Statistical N-gram Model** (n=15) for fast pattern matching**Accuracy Improvements:**

- **Deep Learning LSTM** (2 layers, 256 units) for contextual understanding- `return a +` â†’ `b` (37.32% vs 15.4% before) âœ¨

- **Smart Templates** (60+ patterns) for common Python constructs- `def add ( a , b` â†’ `)` (32.35% confidence)

- **Naming Convention Detection** (get_, set_, is_, has_, validate_, calculate_)- Better rare pattern handling with 4 epochs!



### âœ¨ Key Features---



- ğŸ¯ **85.15% Top-5 Accuracy** on Python code completion## ğŸš€ How to Use Your Model

- âš¡ **Real-time Suggestions** (<20ms response time)

- ğŸ§  **Smart Code Completion** - Understands intent and generates full code blocks### 1. Quick Test (Run Examples)

- ğŸ¨ **VS Code Dark Theme UI** - Beautiful web interface```powershell

- ğŸ”„ **Hybrid Architecture** - Best of statistical and neural approachescd E:\storage\code_sugesstion_system\code-suggestion-ngram

- ğŸ“Š **50,000 Token Vocabulary** - Comprehensive Python coveragepython use_model.py

```

## ğŸ¯ Live DemoThis will show example usage and results.



```python### 2. Interactive Mode

# Type: def add```powershell

# Get:python use_model.py interactive

def add(a, b):```

    """Add two numbers"""Then type code to get suggestions:

    return a + b```

>>> def add ( a , b

# Type: def get_username  Suggestions:

# Get:    )               32.35%

def get_username(self):    ,                7.53%

    """Get username"""

    return self._username>>> for i in range (

  Suggestions:

# Type: try:    len             14.54%

# Get:    n                5.76%

try:

    # Code that might raise exception>>> complete return a +

    pass  Completed: return a + b

except Exception as e:

    # Handle exception>>> quit

    print(f"Error: {e}")```

```

### 3. Use in Your Python Code

## ğŸ“Š Model Performance```python

from src.ngram.ast_tokenizer import ASTTokenizer, VocabularyManager

| Metric | Value |from src.ngram.enhanced_model import EnhancedNGramModel

|--------|-------|from src.ngram.lstm_model import LSTMTrainer

| **Top-1 Accuracy** | 57.97% |from src.ngram.hybrid_model import HybridCodeCompleter

| **Top-3 Accuracy** | 78.68% |

| **Top-5 Accuracy** | 85.15% |# Load model

| **Training Loss** | 1.128 |vocab = VocabularyManager()

| **Validation Loss** | 3.886 |vocab.load('data/processed/vocabulary_best.pkl')

| **Training Time** | 51 minutes (4 epochs, GPU) |

| **Model Parameters** | 26.7M (LSTM) + 1.2M contexts (N-gram) |ngram_model = EnhancedNGramModel(n=15)

| **Dataset Size** | 18,612 Python code samples |ngram_model.load('data/processed/ngram_best_model.pkl')



## ğŸ—ï¸ Architecturelstm_model = LSTMTrainer.load_model('data/processed/lstm_best_model.pth')



```tokenizer = ASTTokenizer()

User Input: "def add(a, b"

     â†“# Create completer

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”completer = HybridCodeCompleter(

â”‚   Tokenization     â”‚    ngram_model=ngram_model,

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    lstm_model=lstm_model,

     â†“    vocabulary=vocab,

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    tokenizer=tokenizer

â”‚  Hybrid Model (Ensemble)   â”‚)

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

â”‚  N-gram    â”‚     LSTM      â”‚# Get suggestions

â”‚  (60%)     â”‚     (40%)     â”‚code = "def calculate ( x , y"

â”‚            â”‚               â”‚suggestions = completer.suggest(code, top_k=5)

â”‚ â€¢ n=15     â”‚ â€¢ 2 layers    â”‚

â”‚ â€¢ 1.2M ctx â”‚ â€¢ 256 units   â”‚for token, prob in suggestions:

â”‚ â€¢ Fast     â”‚ â€¢ Deep        â”‚    print(f"{token}: {prob:.2%}")

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜```

     â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”---

â”‚ Smart Templates    â”‚

â”‚ (60+ patterns)     â”‚## ğŸ“¦ Model Files

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â†“All saved in `data/processed/`:

Top-5 Suggestions + Full Code Block- `vocabulary_best.pkl` - Token vocabulary (50,000 tokens)

```- `ngram_best_model.pkl` - N-gram model (n=15, 1.2M contexts)

- `lstm_best_model.pth` - LSTM model (26.7M parameters)

### Components- `model_info.json` - Model metadata and configuration



1. **N-gram Model** (Statistical)---

   - Size: 15-gram (looks at last 15 tokens)

   - Contexts: 1,210,756 unique patterns## ğŸ¨ API Reference

   - Speed: <1ms per prediction

   - Use case: Common patterns, exact matches### `completer.suggest(code, top_k=5, use_lstm=True)`

Get top-k suggestions for partial code.

2. **LSTM Model** (Neural Network)- `code`: Partial Python code (string)

   - Architecture: 2-layer stacked LSTM- `top_k`: Number of suggestions to return

   - Hidden units: 256 per layer- `use_lstm`: Use hybrid model (True) or N-gram only (False)

   - Embedding: 256 dimensions- Returns: List of (token, probability) tuples

   - Parameters: 26,702,672

   - Context: 50 tokens### `completer.complete_code(code, max_tokens=10)`

   - Use case: Rare patterns, semantic understandingAuto-complete code by generating multiple tokens.

- `code`: Partial Python code

3. **Smart Completer** (Template-based)- `max_tokens`: Maximum tokens to generate

   - Templates: 60+ Python patterns- Returns: Completed code string

   - Naming conventions: 6 patterns (get_, set_, is_, etc.)

   - Confidence scoring: Multi-factor analysis### `completer.get_multiple_completions(code, num_completions=3)`

   - Use case: Full code block generationGet multiple possible completions.

- `code`: Partial Python code

## ğŸš€ Quick Start- `num_completions`: Number of different completions

- Returns: List of completed code strings

### Prerequisites

---

```bash

# Python 3.11+## ğŸ’¡ Tips

# CUDA-capable GPU (optional, for training)

```**For best results:**

1. Provide enough context (5-10 tokens minimum)

### Installation2. Use hybrid mode for rare patterns

3. Use N-gram only mode (`use_lstm=False`) for speed

```bash4. The model understands Python syntax, keywords, and common patterns

# 1. Clone the repository

git clone https://github.com/yourusername/code-suggestion-ngram.git**Performance:**

cd code-suggestion-ngram- N-gram only: Very fast (<1ms)

- Hybrid mode: Fast (~10-50ms on GPU)

# 2. Create virtual environment- Best for: function signatures, loops, conditions, imports

python -m venv venv

source venv/bin/activate  # On Windows: venv\Scripts\activate---



# 3. Install dependencies## ğŸ“Š Model Performance

pip install -r requirements.txt

```**Test Results (4 epochs):**

- Function completion: âœ… Excellent (32-37% confidence)

### Download Pre-trained Models- Loop constructs: âœ… Very good (14-38% confidence)

- Imports: âœ… Smart suggestions (numpy, pandas, requests)

**Option 1: Download from Release**- Return statements: âœ… Context-aware (37% for `return a + b`)

```bash

# Download models from GitHub Releases page**vs 1 Epoch:**

# Extract to production/models/ directory- `return a +` â†’ `b`: **37.32%** (was 15.4%) - **2.4x better!**

```- More confident predictions overall

- Better rare pattern handling

**Option 2: Train Your Own** (See Training section below)

---

### Run Web Application

## ğŸ¯ Next Steps

```bash

# Start the web server1. **Test it:** `python use_model.py`

cd production2. **Try interactive mode:** `python use_model.py interactive`

python web_app.py3. **Integrate in your IDE/editor**

4. **Fine-tune on your own code** (optional)

# Open browser at http://localhost:5000

```Enjoy your accurate code suggestion system! ğŸš€


## ğŸ“ Project Structure

```
code-suggestion-ngram/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”‚
â”œâ”€â”€ training/                    # Training scripts and data
â”‚   â”œâ”€â”€ train_model.py          # Main training script
â”‚   â””â”€â”€ dataset/                # Training dataset
â”‚       â””â”€â”€ README.md           # Dataset download instructions
â”‚
â”œâ”€â”€ evaluation/                  # Model evaluation
â”‚   â”œâ”€â”€ test_model.py           # Test trained models
â”‚   â”œâ”€â”€ compare_models.py       # Compare different models
â”‚   â””â”€â”€ results/                # Evaluation results
â”‚
â”œâ”€â”€ production/                  # Production-ready application
â”‚   â”œâ”€â”€ web_app.py              # Flask web server
â”‚   â”œâ”€â”€ templates/              # HTML templates
â”‚   â”‚   â””â”€â”€ index.html          # Web UI
â”‚   â”œâ”€â”€ models/                 # Trained models (gitignored)
â”‚   â”‚   â”œâ”€â”€ vocabulary_best.pkl
â”‚   â”‚   â”œâ”€â”€ ngram_best_model.pkl
â”‚   â”‚   â”œâ”€â”€ lstm_best_model.pth
â”‚   â”‚   â””â”€â”€ model_info.json
â”‚   â””â”€â”€ src/                    # Source code
â”‚       â”œâ”€â”€ ngram/              # Model implementations
â”‚       â”‚   â”œâ”€â”€ model.py        # N-gram model
â”‚       â”‚   â”œâ”€â”€ lstm_model.py   # LSTM model
â”‚       â”‚   â”œâ”€â”€ hybrid_model.py # Hybrid ensemble
â”‚       â”‚   â”œâ”€â”€ smart_completer.py  # Template system
â”‚       â”‚   â”œâ”€â”€ trainer.py      # Training logic
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ utils/              # Utility functions
â”‚
â””â”€â”€ docs/                        # Documentation
    â”œâ”€â”€ IMPROVEMENTS.md         # v2.0 improvements
    â””â”€â”€ SMART_FEATURES.md       # Smart completion guide
```

## ğŸ“ Training Your Own Model

### 1. Prepare Dataset

```bash
# Download Kaggle Python dataset
# Place in training/dataset/kaggle_python_dataset.json
```

Dataset format:
```json
[
  {
    "instruction": "Write a function to add two numbers",
    "input": "",
    "output": "def add(a, b):\n    return a + b"
  }
]
```

### 2. Train Models

```bash
cd training
python train_model.py
```

Training will:
- Filter Python-only code (removes C++/Java)
- Build 50K vocabulary
- Train 15-gram N-gram model (~5 seconds)
- Train 2-layer LSTM model (~50 minutes on GPU)
- Save models to `production/models/`

**Training Configuration:**
```python
{
  "n": 15,                  # N-gram size
  "vocab_size": 50000,      # Vocabulary size
  "hidden_dim": 256,        # LSTM hidden units
  "layers": 2,              # LSTM layers
  "epochs": 4,              # Training epochs
  "batch_size": 32,         # Mini-batch size
  "learning_rate": 0.001,   # Adam optimizer LR
  "dropout": 0.2           # Dropout rate
}
```

### 3. Evaluate

```bash
cd evaluation
python test_model.py
```

## ğŸ§ª Testing

### Interactive Testing

```bash
cd production
python web_app.py
# Open http://localhost:5000
```

### Command-line Testing

```bash
cd evaluation
python test_model.py --model ../production/models/lstm_best_model.pth
```

### Compare Models

```bash
cd evaluation
python compare_models.py
```

## ğŸ¨ Web Interface Features

### 1. Token-by-Token Suggestions
- Type code and get real-time suggestions
- Use **Tab** to accept suggestion
- Use **â†‘â†“** arrows to navigate suggestions
- Use **Esc** to dismiss

### 2. Smart Mode (ğŸ§  Button)
- Detects user intent
- Generates complete code blocks
- Shows confidence scores
- Press **Ctrl+Enter** to accept

### 3. LSTM Toggle
- Switch between N-gram only and Hybrid mode
- Blue = Hybrid (LSTM + N-gram)
- Gray = N-gram only

### 4. Keyboard Shortcuts

| Shortcut | Action |
|----------|--------|
| **Tab** | Accept token suggestion |
| **â†‘â†“** | Navigate suggestions |
| **Esc** | Dismiss suggestions |
| **Ctrl+Enter** | Accept smart completion |

## ğŸ“Š Technical Details

### Machine Learning Concepts Used

1. **Natural Language Processing**
   - Tokenization (AST-aware)
   - Vocabulary building (50K tokens)
   - Sequence modeling

2. **Statistical ML**
   - N-gram language model
   - Maximum likelihood estimation
   - Add-k smoothing (Laplace)
   - Perplexity metric

3. **Deep Learning**
   - LSTM (Long Short-Term Memory)
   - Word embeddings (256D)
   - Dropout regularization (0.2)
   - Cross-entropy loss

4. **Training Techniques**
   - Mini-batch gradient descent
   - Adam optimizer
   - Early stopping
   - Train/validation split (90/10)
   - GPU acceleration (CUDA)

5. **Ensemble Methods**
   - Weighted ensemble (60% N-gram, 40% LSTM)
   - Hybrid prediction

### Model Files

| File | Size | Description |
|------|------|-------------|
| `vocabulary_best.pkl` | ~3 MB | 50K token dictionary |
| `ngram_best_model.pkl` | ~150 MB | 1.2M N-gram contexts |
| `lstm_best_model.pth` | ~105 MB | 26.7M LSTM parameters |
| `model_info.json` | ~1 KB | Model metadata |

**Total:** ~260 MB (too large for GitHub - use Git LFS or download separately)

## ğŸ› ï¸ Development

### Requirements

```
Python >= 3.11
torch >= 2.5.1
flask >= 3.1.0
numpy >= 1.26.0
```

See `requirements.txt` for full list.

### Running Tests

```bash
# Test individual components
cd evaluation
python test_model.py --verbose
```

### Code Style

- PEP 8 compliant
- Type hints included
- Docstrings for all functions

## ğŸ“ˆ Performance Optimization

### Speed
- N-gram lookups: <1ms
- LSTM inference: ~10ms
- Total response: <20ms (production)

### Memory
- Model size: 260 MB
- Runtime RAM: 300-400 MB
- GPU VRAM: 500 MB (if using CUDA)

### Accuracy Improvements
- Use larger dataset (current: 18K samples)
- Increase vocabulary size (current: 50K)
- Add more LSTM layers (current: 2)
- Train longer (current: 4 epochs)
- Fine-tune weights (current: 60% N-gram, 40% LSTM)

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Dataset:** [Kaggle Python Code Instruction Dataset](https://www.kaggle.com/datasets/thedevastator/python-code-instruction-dataset)
- **Framework:** PyTorch 2.5.1
- **Web Framework:** Flask 3.1.0
- **UI Inspiration:** VS Code Dark+ Theme

## ğŸ“ Contact

For questions or feedback:
- Open an issue on GitHub
- Email: aarths123@gmail.com

## ğŸ¯ Roadmap

- [ ] Support for more programming languages (JavaScript, Java, C++)
- [ ] Transformer-based model (BERT, GPT)
- [ ] VS Code extension
- [ ] API endpoint for integration
- [ ] Mobile app
- [ ] Real-time collaborative coding
- [ ] User-defined custom templates
- [ ] Learning from user corrections

## ğŸ“š Documentation

- [Smart Features Guide](docs/SMART_FEATURES.md) - Complete guide to smart completion
- [Improvements v2.0](docs/IMPROVEMENTS.md) - Latest enhancements and features

## â­ Star History

If you find this project useful, please consider giving it a star!

---

**Made with â¤ï¸ and Python**
